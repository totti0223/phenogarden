{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/totti0223/phenogarden/blob/main/modules/001_rice_panicle/main.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq onnxruntime-gpu supervision\n",
    "\n",
    "!git clone https://github.com/SUNJHZAU/EOPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if onnxruntime can use gpu\n",
    "import onnxruntime as ort\n",
    "ort.get_available_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "\n",
    "from skimage.morphology import skeletonize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grain Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on https://github.com/SUNJHZAU/EOPT\n",
    "class YOLOv8:\n",
    "    \"\"\"\n",
    "    YOLOv8による物体検出を行うクラス。\n",
    "    推論と後処理を担当する。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, onnx_model: str, confidence_thres: float, iou_thres: float):\n",
    "        \"\"\"\n",
    "        YOLOv8のインスタンスを初期化する。\n",
    "\n",
    "        Args:\n",
    "            onnx_model (str): ONNXモデルのパス。\n",
    "            confidence_thres (float): 検出の信頼度しきい値。\n",
    "            iou_thres (float): NMS（非最大抑制）のIoUしきい値。\n",
    "        \"\"\"\n",
    "        self.onnx_model = onnx_model\n",
    "        self.confidence_thres = confidence_thres\n",
    "        self.iou_thres = iou_thres\n",
    "\n",
    "        # ONNXモデルを用いて推論セッションを作成。CUDAとCPUの両方を利用する。\n",
    "        self.session = ort.InferenceSession(self.onnx_model,\n",
    "                                            providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "        self.model_inputs = self.session.get_inputs()\n",
    "\n",
    "        # モデルの入力サイズを取得する\n",
    "        input_shape = self.model_inputs[0].shape\n",
    "        self.input_width = input_shape[2]\n",
    "        self.input_height = input_shape[3]\n",
    "\n",
    "    def preprocess(self, img_path: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        画像を推論前に前処理する。\n",
    "\n",
    "        Args:\n",
    "            img_path (str): 入力画像のパス。\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 推論用に前処理された画像データ。\n",
    "        \"\"\"\n",
    "        # OpenCVで画像を読み込み、元画像とそのサイズを保持する\n",
    "        self.img = cv2.imread(img_path)\n",
    "        self.img_height, self.img_width = self.img.shape[:2]\n",
    "\n",
    "        # BGRからRGBに変換し、モデルの入力サイズにリサイズする\n",
    "        img_rgb = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n",
    "        resized_img = cv2.resize(img_rgb, (self.input_width, self.input_height))\n",
    "\n",
    "        # 画像を0～1の範囲に正規化し、チャンネルファーストに変換する\n",
    "        image_data = np.array(resized_img) / 255.0\n",
    "        image_data = np.transpose(image_data, (2, 0, 1))\n",
    "        image_data = np.expand_dims(image_data, axis=0).astype(np.float32)\n",
    "\n",
    "        return image_data\n",
    "\n",
    "    def postprocess(self, output: list) -> list:\n",
    "        \"\"\"\n",
    "        モデルの出力から検出結果（バウンディングボックス、信頼度、クラスID）を抽出する。\n",
    "\n",
    "        Args:\n",
    "            output (list): モデルの推論結果。\n",
    "\n",
    "        Returns:\n",
    "            list: 検出結果の辞書リスト。各辞書は 'box', 'score', 'clsID' を含む。\n",
    "        \"\"\"\n",
    "        # 出力を整形する（余分な次元を除去し転置する）\n",
    "        outputs = np.transpose(np.squeeze(output[0]))\n",
    "        num_rows = outputs.shape[0]\n",
    "\n",
    "        boxes, scores, class_ids = [], [], []\n",
    "        x_scale = self.img_width / self.input_width\n",
    "        y_scale = self.img_height / self.input_height\n",
    "\n",
    "        # 各出力行について処理する\n",
    "        for i in range(num_rows):\n",
    "            row = outputs[i]\n",
    "            class_scores = row[4:]\n",
    "            max_score = np.amax(class_scores)\n",
    "\n",
    "            # 信頼度が閾値以上の検出のみ採用する\n",
    "            if max_score >= self.confidence_thres:\n",
    "                cls_id = int(np.argmax(class_scores))\n",
    "                x_center, y_center, w, h = row[0], row[1], row[2], row[3]\n",
    "                left = int((x_center - w / 2) * x_scale)\n",
    "                top = int((y_center - h / 2) * y_scale)\n",
    "                right = int((x_center + w / 2) * x_scale)\n",
    "                bottom = int((y_center + h / 2) * y_scale)\n",
    "                # width = int(w * x_scale)\n",
    "                # height = int(h * y_scale)\n",
    "                #boxes.append([left, top, width, height])\n",
    "                # record in xyxy format instead\n",
    "                boxes.append([left,top,right,bottom])\n",
    "                scores.append(float(max_score))\n",
    "                class_ids.append(cls_id)\n",
    "\n",
    "        # 非最大抑制を適用して重複検出を排除する\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, scores, self.confidence_thres, self.iou_thres)\n",
    "        results = []\n",
    "\n",
    "        # NMSBoxesが返す形式に注意（場合によっては [[idx], ...] の形式）\n",
    "        if len(indices) > 0:\n",
    "            indices = indices.flatten()  # フラットな配列に変換する\n",
    "            for idx in indices:\n",
    "                results.append({\n",
    "                    'xyxy': boxes[idx],\n",
    "                    'scores': scores[idx],\n",
    "                    'class_id': class_ids[idx]\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __call__(self, img_path: str) -> list:\n",
    "        \"\"\"\n",
    "        画像パスを与えると、前処理・推論・後処理を実行し検出結果を返す。\n",
    "\n",
    "        Args:\n",
    "            img_path (str): 入力画像のパス。\n",
    "\n",
    "        Returns:\n",
    "            list: 検出結果の辞書リスト。\n",
    "        \"\"\"\n",
    "        # 前処理、推論、後処理の各工程を順に実施する\n",
    "        img_data = self.preprocess(img_path)\n",
    "        outputs = self.session.run(None, {self.model_inputs[0].name: img_data})\n",
    "        return self.postprocess(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = YOLOv8('GrainNuber.onnx', 0.1, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/content/EOPT/show/cut/1269.jpg\"\n",
    "result = detector(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_yolov8(cls, result):\n",
    "    \"\"\"\n",
    "    Creates a Detections instance from yolov8. monkey patch for supervision.\n",
    "    \"\"\"\n",
    "    return cls(\n",
    "        xyxy=np.array([x[\"xyxy\"] for x in result]),\n",
    "        confidence=np.array([x[\"scores\"] for x in result]),\n",
    "        class_id=np.array([x[\"class_id\"] for x in result]),\n",
    "        tracker_id=None,\n",
    "        data={\"class_name\": np.array([\"grain\"]*len(result))}\n",
    "    )\n",
    "sv.Detections.from_yolov8 = classmethod(from_yolov8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(path)\n",
    "result = detector(path)\n",
    "\n",
    "orig_detections = sv.Detections.from_yolov8(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = copy.deepcopy(orig_detections[orig_detections.confidence>0.5])\n",
    "\n",
    "# bounding_box_annotator = sv.BoxAnnotator()\n",
    "bounding_box_annotator = sv.BoxCornerAnnotator()#sv.RoundBoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    text_scale=0.5\n",
    ")\n",
    "\n",
    "labels = [\n",
    "    f\"{confidence:.2f}\"\n",
    "    for confidence\n",
    "    in detections.confidence\n",
    "]\n",
    "\n",
    "annotated_image = bounding_box_annotator.annotate(\n",
    "    scene=image.copy(),\n",
    "    detections=detections,\n",
    ")\n",
    "annotated_image = label_annotator.annotate(\n",
    "    scene=annotated_image, detections=detections, labels=labels)\n",
    "\n",
    "annotated_image = sv.draw_text(\n",
    "    scene=annotated_image, text=str(len(detections.xyxy)),\n",
    "    text_anchor=sv.Point(x=200,y=100),\n",
    "    text_color=sv.Color(255,255,255),\n",
    "    text_scale=5,\n",
    "    text_thickness=10)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(annotated_image[...,::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
